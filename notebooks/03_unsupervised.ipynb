{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_5j8yQ1K7S4"
   },
   "source": [
    "# Week 03: Dimensionality Reduction and Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOKx0Y0pLIfB"
   },
   "source": [
    "## Text as Data\n",
    "\n",
    "Professor: Elliott Ash, NYU\n",
    "\n",
    "TA: Eduardo Zago, NYU\n",
    "\n",
    "More accurate objective of the course: learning how to use text as data for research objectives, while also trying to understand how to build an LLM from scratch.\n",
    "\n",
    "What have we done so far:\n",
    "\n",
    "1.   Introduction to tools to manage text in Python\n",
    "2.   Preprocessing of text\n",
    "3.   Tokenization of text (encoder-decoder algorithms)\n",
    "\n",
    "Now, how do we represent this tokens mathematically? And what can we do with this representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32789,
     "status": "ok",
     "timestamp": 1770592028784,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "SDJ1hPoLMtkr",
    "outputId": "8fcbf444-d137-4b65-d91c-095ecf6dd4dd"
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "\n",
    "!pip install gensim\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 24552,
     "status": "ok",
     "timestamp": 1770590786048,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "_T9Q2nCeR-0B",
    "outputId": "ad346839-a1f6-4d20-9c31-1a6efbc1db02"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1770592037868,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "dEJGME5QSXw4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sc_cases_cleaned.pkl',\n",
    "                    compression = 'gzip')\n",
    "\n",
    "# Basic preprocessing for the dataset\n",
    "translator = str.maketrans(' ', ' ', punctuation)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(doc):\n",
    "  doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "  doc = re.sub(r\"(\\d)([A-Za-z])\", r\"\\1 \\2\", doc) # separate numbers from strings\n",
    "  doc = re.sub(r\"([A-Za-z])(\\d)\", r\"\\1 \\2\", doc) # separate strings from numbers\n",
    "  d = doc.translate(translator).lower() # remove punctuation\n",
    "  words = word_tokenize(d)\n",
    "  words = [w for w in words if w not in stoplist] # remove stopwords\n",
    "  words = [w if not w.isdigit() else '#' for w in words] # normalize numbers\n",
    "  output = ' '.join(words) # Let's not tokenize now\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx43IyXMT8VF"
   },
   "source": [
    "Last lab we introduced how one would represent mathemathically a corpus: term document matrix X, where\n",
    "\n",
    "1. rows = documents\n",
    "2. columns = tokens (words or n-grams)\n",
    "3. values = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "executionInfo": {
     "elapsed": 35349,
     "status": "ok",
     "timestamp": 1770592075784,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "z2f6mQitSjCx",
    "outputId": "c6e67429-6703-47e0-e5d1-9d54991eda31"
   },
   "outputs": [],
   "source": [
    "preprocessed_opinion = list(map(preprocess, df['opinion_text'])) # Note list()\n",
    "\n",
    "# Generate a date - judge index\n",
    "df['index'] = df['authorship'] + df['date_standard'].astype(str)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(min_df=0.01,\n",
    "                      max_df=.9,\n",
    "                      max_features=1000)\n",
    "\n",
    "X = vec.fit_transform(preprocessed_opinion)\n",
    "\n",
    "vocab_opinions = vec.get_feature_names_out()\n",
    "X_lab = pd.DataFrame(X.toarray(), columns=vocab_opinions, index=df['index']) # only for didactic purposes, keep only the X\n",
    "\n",
    "X_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkEg7J4gbd9m"
   },
   "source": [
    "Different ways of measuring similarity across text. The first one that comes to mind is the Euclidean distance:\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix}, \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "$$\\|\\mathbf{x}-\\mathbf{y}\\| = \\sqrt{\\sum_{i=1}^n\\left(x_i-y_i\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1770591236759,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "YVdVa7lzYEo7",
    "outputId": "680fe528-6e7c-450f-85d6-309fb4a42fc5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances #NEW\n",
    "\n",
    "euclid = euclidean_distances(X_lab) # Computes the pairwise Euclidean distance between all rows. this is\n",
    "\n",
    "print(euclid[0,2]) # What is this? ERASE: the Euclidean distance between document 0 and document 2 in the vector space defined by the document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7nAJLtviy4R"
   },
   "source": [
    "### Cosine\n",
    "\n",
    "$$\\cos θ = \\frac{\\mathbf{x}^{\\top}\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1770413305922,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "Fw00mR8tiT5C",
    "outputId": "553ddd09-5649-4fb5-8363-04a0cef107dc"
   },
   "outputs": [],
   "source": [
    "cos = cosine_similarity(X_lab)\n",
    "\n",
    "print(cos[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm6DT-kKdrmR"
   },
   "source": [
    "Cosine similarity is ubiquitous in NLP because almost everything reduces to comparing vectors, and cosine is a simple, scale-invariant, and empirically effective way to compare vector meaning.\n",
    "\n",
    "Why cosine?\n",
    "\n",
    "1) Ignores absolute magnitude\n",
    "\n",
    "2) Focuses on direction (semantic content)\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1770413312799,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "-vLKkKAAfKzP",
    "outputId": "9de922a8-fc5d-4c49-aa22-a6b169496450"
   },
   "outputs": [],
   "source": [
    "X_aug = X_lab.copy()\n",
    "X_aug.loc[\"last_x4\"] = 4 * X_lab.iloc[-1]\n",
    "\n",
    "# 2) compare similarities/distances\n",
    "cos = cosine_similarity(X_aug)\n",
    "euc = euclidean_distances(X_aug)\n",
    "\n",
    "print(\"Cosine similarity (last vs 4x-last):\", cos[len(X_aug) - 2, len(X_aug) - 1])\n",
    "print(\"Euclidean distance (last vs 4x-last):\", euc[len(X_aug) - 2, len(X_aug) - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d10jFvbmkKvF"
   },
   "source": [
    "### tf-idf\n",
    "\n",
    "$$tfidf_{t,d} = tf_{t,d} \\times \\log\\left(\\frac{N}{df_t}\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$tf_{t,d} = \\frac{\\text{count of } t \\text{ in } d}{\\sum_{t'} \\text{count of } t' \\text{ in } d}.$$\n",
    "\n",
    "Key: words that are frequent in a specific document but rare in the corpus receive higher weights, improving their usefulness for representing document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1770413322362,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "p5qV3pSImDCO",
    "outputId": "2c6c10ce-678c-44dc-faf6-0e8af68e786c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # NEW\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=0.01,\n",
    "                        max_df=0.9,\n",
    "                        max_features=1000)\n",
    "\n",
    "X = tfidf.fit_transform(preprocessed_opinion)\n",
    "\n",
    "X_tfidf = pd.DataFrame(X.toarray(), columns=vocab_opinions, index=df['index'])\n",
    "\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq6kM97UngF3"
   },
   "source": [
    "### Co-Ocurrence (term-term matrices)\n",
    "\n",
    "Like documents, terms can also be represented using counts from a corpus. One popular method is to 'count the neighbors,' which surprisingly captures many properties about the word.\n",
    "\n",
    "Note: \"Co-occurrence matrix\" is more often used in NLP, but \"term-term matrix\" might be more intuitive to think about in relationship to \"term-document matrix.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4591,
     "status": "ok",
     "timestamp": 1770592080380,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "IfJ97y9pneeZ"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "gins = df[df.iloc[:, 3]=='GINSBURG']\n",
    "preprocess_gins = list(map(preprocess, gins['opinion_text']))\n",
    "\n",
    "# Adapted from https://www.geeksforgeeks.org/co-occurence-matrix-in-nlp/\n",
    "# Input: list of strings, window size\n",
    "def get_ttm(corpus, window_size):\n",
    "  # Create a list of co-occurring word pairs\n",
    "  co_occurrences = defaultdict(Counter)\n",
    "  all_words = []\n",
    "\n",
    "  for article in corpus:\n",
    "    words = article.split(\" \")\n",
    "    all_words += words\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i-window_size), min(len(words), i+window_size+1)):\n",
    "            if i != j:\n",
    "                co_occurrences[word][words[j]] += 1\n",
    "\n",
    "  # Create a list of unique words\n",
    "  unique_words = list(set(all_words))\n",
    "\n",
    "  # Initialize the co-occurrence matrix\n",
    "  co_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
    "\n",
    "  # Populate the co-occurrence matrix\n",
    "  word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "  for word, neighbors in co_occurrences.items():\n",
    "      for neighbor, count in neighbors.items():\n",
    "          co_matrix[word_index[word]][word_index[neighbor]] = count\n",
    "\n",
    "  # Create a DataFrame for better readability\n",
    "  co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "  # Return the co-occurrence matrix and word index mapping\n",
    "  return co_matrix_df, word_index\n",
    "\n",
    "\n",
    "# get TTM with window size 1\n",
    "opinion_ttm_w1, opinion_w2i_w1 = get_ttm(preprocess_gins, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1770592096927,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "r8VO9TrQdmpS",
    "outputId": "f543a90c-bf0e-4ad8-9e3f-3e431bf35d17"
   },
   "outputs": [],
   "source": [
    "opinion_ttm_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KCdw2wZa1wq"
   },
   "source": [
    "Note 2: Representations in co-occurrence matrices become more reliable with more text data (i.e. larger corpora). Because the corpus we're using today is somewhat small, the results from the example below might seem somewhat unintuitive. The main goal should be to get an idea of how to build a co-occurrence matrix and calculate similarity over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_FK6ADxorgW"
   },
   "source": [
    "### Applications:\n",
    "\n",
    "1.   Clustering (K-Means, DBSCAN, PCA)\n",
    "2.   Topic Modelling (LDA, Structural Topic Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmwRtdzEpiMy"
   },
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohIaKDzPo_oX"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #NEW\n",
    "\n",
    "#\n",
    "num_clusters = 20 # Optimal number of clusters\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(X_tfidf)\n",
    "\n",
    "doc_clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1770413863226,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "ov8FvfQQ4a2K",
    "outputId": "5d5ab98f-6589-4c11-ba63-cab98bdacdd5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_tfidf, km.labels_)\n",
    "\n",
    "sil_scores = []\n",
    "for n in range(2, num_clusters):\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(X_tfidf)\n",
    "    sil_scores.append(silhouette_score(X_tfidf, km.labels_))\n",
    "\n",
    "opt_sil_score = max(sil_scores[5:20])\n",
    "sil_scores.index(opt_sil_score)\n",
    "opt_num_cluster = range(2, num_clusters)[sil_scores.index(opt_sil_score)]\n",
    "print('The optimal number of clusters is %s' %opt_num_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e0EXSIRpkeF"
   },
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1770416729854,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "vY0ml1SDpmxN",
    "outputId": "8f6f86d9-72f8-487e-a045-2f8d8ef02edf"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN # NEW\n",
    "\n",
    "dbscan = DBSCAN(eps=0.95, min_samples=5)\n",
    "dbscan.fit(X_tfidf)\n",
    "db_clusters = dbscan.labels_\n",
    "\n",
    "df['cluster_db'] = db_clusters\n",
    "df[df['cluster_db']==1]['opinion_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS90Rf3vpoge"
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTXYOBScpsGD"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # NEW\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "\n",
    "pca_tfidf.fit(X_tfidf)\n",
    "\n",
    "cumvar_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1770414876422,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "cGVnYxNdrPA-",
    "outputId": "ddf1fdf5-f7f1-4473-cfa3-6b1d54a5cb16"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1, len(cumvar_tfidf)+1), cumvar_tfidf, marker='o', label=\"X_tfidf\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot: TF-IDF\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4wtO-lL8zxP"
   },
   "source": [
    "Is this a good elbow plot? What is the conclusion that we get from this?\n",
    "\n",
    "With text data, elbows are often weak or smooth (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3ZfJwovpssZ"
   },
   "source": [
    "### Topic Modelling\n",
    "\n",
    "#### LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4833,
     "status": "ok",
     "timestamp": 1770418451861,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "LrdQqDIYreMX",
    "outputId": "1c99b256-bfd3-40fd-fe86-b724792ee6f8"
   },
   "outputs": [],
   "source": [
    "# split into paragraphs\n",
    "doc_clean = []\n",
    "for doc in preprocessed_opinion:\n",
    "    # split by paragraph\n",
    "    for paragraph in doc.split(\"\\n\\n\"):\n",
    "        doc_clean.append(doc.split())\n",
    "print(doc_clean[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3058,
     "status": "ok",
     "timestamp": 1770418482464,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "lU-yStFaKhRd",
    "outputId": "39b38561-de3c-4e2e-b10d-f904c6bdbe30"
   },
   "outputs": [],
   "source": [
    "# randomize document order\n",
    "from random import shuffle\n",
    "shuffle(doc_clean)\n",
    "\n",
    "# creating the term dictionary\n",
    "from gensim import corpora # New\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# filter extremes, drop all words appearing in less than 10 paragraphs and all words appearing in at least every third paragraph\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.33, keep_n=1000)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16369,
     "status": "ok",
     "timestamp": 1770419783783,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "siPBfXSKJGOL",
    "outputId": "d9fcbba1-fada-44b9-f818-82dcc9ef5cab"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"datetime.datetime.utcnow\",\n",
    "    category=DeprecationWarning\n",
    ")\n",
    "\n",
    "# creating the document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# train LDA with 10 topics and print\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(doc_term_matrix, num_topics=4,\n",
    "               id2word = dictionary, passes=3)\n",
    "lda.show_topics(formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 4892,
     "status": "ok",
     "timestamp": 1770419176993,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "q2Ozt_rbrozh",
    "outputId": "e7c10416-ed7a-4733-c266-bdf004f1732f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 341420,
     "status": "error",
     "timestamp": 1770419725492,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "Ym1fJPagr5mM",
    "outputId": "ea1a3a03-008a-43b9-cedb-fbfbc17e2231"
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherence_scores = []\n",
    "for k in range(1, 10):\n",
    "    lda = LdaModel(\n",
    "        corpus=doc_term_matrix,\n",
    "        num_topics=k,\n",
    "        id2word=dictionary,\n",
    "        passes=3,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    cm = CoherenceModel(\n",
    "        model=lda,\n",
    "        texts=doc_clean,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "\n",
    "    coherence_scores.append(cm.get_coherence())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1770419748653,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "4GNBIXu6PXMy",
    "outputId": "3f4cebcb-0eb2-482e-ac2b-848cc40b3e77"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,10), coherence_scores, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "plt.title(\"LDA Topic Coherence (1–10 Topics)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIeZU5SBsAGZ"
   },
   "source": [
    "#### Author Topic Model (Structural Topic Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1770419784390,
     "user": {
      "displayName": "Eduardo Zago",
      "userId": "04499579548658787955"
     },
     "user_tz": 300
    },
    "id": "9GgkkFM4sD31",
    "outputId": "401f6994-2715-4bad-f8a6-56c9bb4224fd"
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import temporary_file\n",
    "\n",
    "df = df.reset_index()\n",
    "df['id'] = df.index\n",
    "author2doc = df[:100][['authorship','id']]\n",
    "author2doc = author2doc.groupby('authorship').apply(lambda x: list(x['id'])).to_dict()\n",
    "\n",
    "model = AuthorTopicModel(\n",
    "        doc_term_matrix, author2doc=author2doc, id2word=dictionary, num_topics=10)\n",
    "\n",
    "# For each author list topic distribution\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "author_vecs[:2]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOjH7AJdDz0nxWkgH1jYdjE",
   "provenance": [
    {
     "file_id": "1s3Cxgi28y78TyVUJtGpa6h6Gf9cWAgnl",
     "timestamp": 1770410203577
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
